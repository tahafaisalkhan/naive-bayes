{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Machine Learning project created in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: nltk in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/tahafaisal/anaconda3/lib/python3.11/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tahafaisal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_data = pd.read_csv('golf_data.csv')\n",
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = load_dataset('tweet_eval', 'emotion', cache_dir=\"datasets\")\n",
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_golf = golf_data.copy()\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "df_golf['Month'] = lb.fit_transform(df_golf['Month'])\n",
    "df_golf['Season'] = lb.fit_transform(df_golf['Season'])\n",
    "df_golf['Temperature'] = lb.fit_transform(df_golf['Temperature'])\n",
    "df_golf['Humidity'] = lb.fit_transform(df_golf['Humidity'])\n",
    "df_golf['Outlook'] = lb.fit_transform(df_golf['Outlook'])\n",
    "df_golf['Crowdedness'] = lb.fit_transform(df_golf['Crowdedness'])\n",
    "\n",
    "y = df_golf['Play']\n",
    "X = df_golf.drop(columns=['Play'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tahafaisal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def processor(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "tweet_train_data = tweet_data['train'].to_pandas()\n",
    "tweet_test_data = tweet_data['test'].to_pandas()\n",
    "tweet_validation_data = tweet_data['validation'].to_pandas()\n",
    "\n",
    "tweet_train_data['cleaned_text'] = tweet_train_data['text'].apply(processor)\n",
    "tweet_test_data['cleaned_text'] = tweet_test_data['text'].apply(processor)\n",
    "tweet_validation_data['cleaned_text'] = tweet_validation_data['text'].apply(processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Naive Bayes from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Bernoulli Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_probs = {}\n",
    "        self.classes = []\n",
    "\n",
    "    def training(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        for i in self.classes:\n",
    "            total_class = np.sum(y == i)\n",
    "            total_count = len(y)\n",
    "            self.class_priors[i] = total_class / total_count\n",
    "\n",
    "        for i in self.classes:\n",
    "            X_c = X[y == i]\n",
    "            fcount = np.sum(X_c, axis=0)\n",
    "            tfcount = X_c.shape[0]\n",
    "            sprob = (fcount + 1) / (tfcount + 2) # Laplace smoothing\n",
    "            self.feature_probs[i] = sprob\n",
    "\n",
    "    def predicting(self, X):\n",
    "        X = np.array(X, dtype=float)\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            x = np.array(x, dtype=float)\n",
    "            class_scores = {}\n",
    "\n",
    "            for i in self.classes:\n",
    "                plog = np.log(self.class_priors[i])\n",
    "                temp = self.feature_probs[i].astype(float)\n",
    "                present_log_likelihood = x * np.log(temp)\n",
    "                temp = self.feature_probs[i].astype(float)\n",
    "                abs_likihood = (1 - x) * np.log(1 - temp)\n",
    "                add = present_log_likelihood + abs_likihood\n",
    "                total_log_likelihood = np.sum(add)\n",
    "                class_scores[i] = plog + total_log_likelihood\n",
    "\n",
    "            best_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(best_class)\n",
    "\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.52\n",
      "Recall: 0.03\n",
      "F1 Score: 0.06\n",
      "Confusion Matrix:\n",
      "[[1872   13]\n",
      " [ 401   14]]\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNaiveBayes()\n",
    "model.training(X_train.values, y_train.values)\n",
    "\n",
    "test_predictions = model.predicting(X_test.values)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, test_predictions)\n",
    "precision_test = precision_score(y_test, test_predictions, average='binary')\n",
    "recall_test = recall_score(y_test, test_predictions, average='binary')\n",
    "f1_test = f1_score(y_test, test_predictions, average='binary')\n",
    "conf_matrix_test = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.2f}\")\n",
    "print(f\"Precision: {precision_test:.2f}\")\n",
    "print(f\"Recall: {recall_test:.2f}\")\n",
    "print(f\"F1 Score: {f1_test:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multinomial Naive Bayes (Manual Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = []\n",
    "        self.word_to_index = {} # did this so I could do the above mapping as shown in the example\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        unique_words = set()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            words = sentence.split()\n",
    "            unique_words.update(words)\n",
    "\n",
    "        self.vocabulary = sorted(list(unique_words))\n",
    "\n",
    "        for idx, word in enumerate(self.vocabulary): # done to map the word to the index as shown in the example\n",
    "            self.word_to_index[word] = idx\n",
    "\n",
    "    def vectorize(self, sentence):\n",
    "        vlen = len(self.vocabulary)\n",
    "        vector = [0] * vlen\n",
    "        words = sentence.split()\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.word_to_index:\n",
    "                index = self.word_to_index[word]\n",
    "                vector[index] += 1\n",
    "\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually set Vocabulary: ['the', 'cat', 'sat', 'on', 'mat']\n",
      "Word to Index Mapping: {'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4}\n",
      "Vectorized Sentence: [2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "bow = BagOfWords()\n",
    "bow.vocabulary = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "for idx, word in enumerate(bow.vocabulary):\n",
    "    bow.word_to_index[word] = idx\n",
    "test_sentence = \"the cat sat on the mat\"\n",
    "vectorized_sentence = bow.vectorize(test_sentence)\n",
    "print(\"Manually set Vocabulary:\", bow.vocabulary)\n",
    "print(\"Word to Index Mapping:\", bow.word_to_index)\n",
    "print(\"Vectorized Sentence:\", vectorized_sentence)\n",
    "\n",
    "corpus = tweet_train_data['cleaned_text'].tolist()\n",
    "bow = BagOfWords()\n",
    "bow.fit(corpus)\n",
    "\n",
    "X_train_vectorized = []\n",
    "X_validation_vectorized = []\n",
    "X_test_vectorized = []\n",
    "\n",
    "for sentence in tweet_train_data['cleaned_text']:\n",
    "    vectorized_sentence = bow.vectorize(sentence)\n",
    "    X_train_vectorized.append(vectorized_sentence)\n",
    "\n",
    "for sentence in tweet_validation_data['cleaned_text']:\n",
    "    vectorized_sentence = bow.vectorize(sentence)\n",
    "    X_validation_vectorized.append(vectorized_sentence)\n",
    "\n",
    "for sentence in tweet_test_data['cleaned_text']:\n",
    "    vectorized_sentence = bow.vectorize(sentence)\n",
    "    X_test_vectorized.append(vectorized_sentence)\n",
    "\n",
    "y_train = tweet_train_data['label'].values\n",
    "y_validation = tweet_validation_data['label'].values\n",
    "y_test = tweet_test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.log_prior = {}\n",
    "        self.log_likelihood = {}\n",
    "        self.vocabulary = set() # did not use BagOfWords class here. Could have alternatively implemented the vectorization within the class but have already done that above\n",
    "        self.classes = []\n",
    "\n",
    "    def training(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        Ndoc = len(X)\n",
    "\n",
    "        for i in self.classes:\n",
    "            Nc = np.sum(y == i)\n",
    "            self.log_prior[i] = np.log(Nc / Ndoc)\n",
    "            docB = X[y == i]\n",
    "            word_counts = np.sum(docB, axis=0)\n",
    "            lth = len(word_counts)\n",
    "            self.vocabulary.update(range(lth))\n",
    "            tote_count = np.sum(word_counts) + len(self.vocabulary)\n",
    "            temp = (word_counts + 1) / tote_count\n",
    "            self.log_likelihood[i] = np.log(temp)\n",
    "\n",
    "    def predicting(self, X):\n",
    "        X = np.array(X)\n",
    "        predictions = []\n",
    "\n",
    "        for doc in X:\n",
    "            class_scores = {}\n",
    "\n",
    "            for j in self.classes:\n",
    "                score = self.log_prior[j]\n",
    "\n",
    "                for i, count in enumerate(doc):\n",
    "                    if i in self.vocabulary:\n",
    "                        score += count * self.log_likelihood[j][i]\n",
    "\n",
    "                class_scores[j] = score\n",
    "\n",
    "            predictions.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "Accuracy: 0.65\n",
      "Precision: 0.75\n",
      "Recall: 0.52\n",
      "F1 Score: 0.54\n",
      "Confusion Matrix:\n",
      "[[141   7   0  12]\n",
      " [ 38  44   0  15]\n",
      " [ 15   2   4   7]\n",
      " [ 29   6   0  54]]\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.training(X_train_vectorized, y_train)\n",
    "validation_predictions = nb.predicting(X_validation_vectorized)\n",
    "accuracy_val = accuracy_score(y_validation, validation_predictions)\n",
    "precision_val = precision_score(y_validation, validation_predictions, average='macro')\n",
    "recall_val = recall_score(y_validation, validation_predictions, average='macro')\n",
    "f1_val = f1_score(y_validation, validation_predictions, average='macro')\n",
    "conf_matrix_val = confusion_matrix(y_validation, validation_predictions)\n",
    "print(f\"Validation Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_val:.2f}\")\n",
    "print(f\"Precision: {precision_val:.2f}\")\n",
    "print(f\"Recall: {recall_val:.2f}\")\n",
    "print(f\"F1 Score: {f1_val:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Accuracy: 0.65\n",
      "Precision: 0.68\n",
      "Recall: 0.53\n",
      "F1 Score: 0.54\n",
      "Confusion Matrix:\n",
      "[[501  19   2  36]\n",
      " [120 173   2  63]\n",
      " [ 74  12  14  23]\n",
      " [120  20   3 239]]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = nb.predicting(X_test_vectorized)\n",
    "accuracy_test = accuracy_score(y_test, test_predictions)\n",
    "precision_test = precision_score(y_test, test_predictions, average='macro')\n",
    "recall_test = recall_score(y_test, test_predictions, average='macro')\n",
    "f1_test = f1_score(y_test, test_predictions, average='macro')\n",
    "conf_matrix_test = confusion_matrix(y_test, test_predictions)\n",
    "print(f\"Test Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.2f}\")\n",
    "print(f\"Precision: {precision_test:.2f}\")\n",
    "print(f\"Recall: {recall_test:.2f}\")\n",
    "print(f\"F1 Score: {f1_test:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Naive Bayes using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (Tweet Data) Metrics on Test Set:\n",
      "Accuracy: 0.65\n",
      "Precision: 0.68\n",
      "Recall: 0.53\n",
      "F1 Score: 0.54\n",
      "Confusion Matrix:\n",
      "[[501  19   2  36]\n",
      " [120 173   2  63]\n",
      " [ 74  12  14  23]\n",
      " [120  20   3 239]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vectorized, y_train)\n",
    "mnb_test_predictions = mnb.predict(X_test_vectorized)\n",
    "\n",
    "accuracy_mnb = accuracy_score(y_test, mnb_test_predictions)\n",
    "precision_mnb = precision_score(y_test, mnb_test_predictions, average='macro')\n",
    "recall_mnb = recall_score(y_test, mnb_test_predictions, average='macro')\n",
    "f1_mnb = f1_score(y_test, mnb_test_predictions, average='macro')\n",
    "conf_matrix_mnb = confusion_matrix(y_test, mnb_test_predictions)\n",
    "\n",
    "print(\"Multinomial Naive Bayes (Tweet Data) Metrics on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy_mnb:.2f}\")\n",
    "print(f\"Precision: {precision_mnb:.2f}\")\n",
    "print(f\"Recall: {recall_mnb:.2f}\")\n",
    "print(f\"F1 Score: {f1_mnb:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_mnb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes (Golf Data) Metrics on Test Set:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.52\n",
      "Recall: 0.03\n",
      "F1 Score: 0.06\n",
      "Confusion Matrix:\n",
      "[[1872   13]\n",
      " [ 401   14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "y = df_golf['Play']\n",
    "X = df_golf.drop(columns=['Play'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "bnb_test_predictions = bnb.predict(X_test)\n",
    "\n",
    "accuracy_bnb = accuracy_score(y_test, bnb_test_predictions)\n",
    "precision_bnb = precision_score(y_test, bnb_test_predictions, average='binary')\n",
    "recall_bnb = recall_score(y_test, bnb_test_predictions, average='binary')\n",
    "f1_bnb = f1_score(y_test, bnb_test_predictions, average='binary')\n",
    "conf_matrix_bnb = confusion_matrix(y_test, bnb_test_predictions)\n",
    "\n",
    "print(\"Bernoulli Naive Bayes (Golf Data) Metrics on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy_bnb:.2f}\")\n",
    "print(f\"Precision: {precision_bnb:.2f}\")\n",
    "print(f\"Recall: {recall_bnb:.2f}\")\n",
    "print(f\"F1 Score: {f1_bnb:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_bnb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
